# VIDY Paper Reading Group 2023-2024 :books:

Welcome to our bi-weekly joint reading group with Dartmouth, UIUC, VT, and Yale. To participate and subscribe to our email update, please kindly contact us. [Website](https://wanghh7.github.io/VIDY-paper-reading-group/)

<table>
  <tr>
    <td align="center">W
      <img src="https://vtnews.vt.edu/global_assets/images/logo-maroon.svg" alt="VT" width="200"/>
    </td>
    <td align="center">
      <img src="https://brand.illinois.edu/wp-content/uploads/2021/09/wordmark.png" alt="UIUC" width="200"/>
    </td>
    <td align="center">
      <img src="https://marvel-b1-cdn.bc0a.com/f00000000283318/communications.dartmouth.edu/sites/communications.prod/files/dpine_lockup.jpg" alt="Dartmouth" width="200"/>
    </td>
    <td align="center">
      <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6e/Yale_University_logo.svg/1280px-Yale_University_logo.svg.png" alt="Yale" width="200"/>
    </td>
  </tr>
</table>

In this repository, you will find the paper list and slidedecks.

----

## Timeline

| Meeting date | Theme        | Topic                                            | Paper                                                        | Slidedeck                                                    | Resources                                                    |
| ------------ | ------------ | ------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2023/10/11   | Algorithm    | Generative models                                | [DiGress: Discrete Denoising diffusion for graph generation](https://arxiv.org/abs/2209.14734) | [Link](./material/20231011-Generative%20models/DiGress.pdf)  | [Link](./material/20231011-Generative%20models/)             |
| 2023/10/25   | Applications | Brain network                                    | [Brain Network Transformer](https://arxiv.org/abs/2210.06681) |                                                              | [Link](./material/20231025-Brain%20network/)                 |
| 2023/11/01   | Algorithm    | Hyperbolic Representation Learning               | [Fully Hyperbolic Neural Networks](https://arxiv.org/abs/2105.14686) | [Link](./material/20231101-Hyperbolic%20Learning/20231101-Paper%20sharing-hyperbolic%20neural%20network.pdf) | [Link](./material/20231101-Hyperbolic%20Learning/)           |
| 2023/11/15   | Algorithm    | Unlearning                                       | [Certified Data Removal from Machine Learning Models](https://arxiv.org/pdf/1911.03030.pdf) |                                                              | [Link](./material/20231115-Unlearning/)                      |
| 2023/12/06   | Algorithm    | Symbolic reasoning and learning                  | [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf), [SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS](https://arxiv.org/pdf/2203.11171.pdf), [LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2205.10625.pdf), [LANGUAGE MODELS ARE MULTILINGUAL CHAIN-OF-THOUGHT REASONERS](https://arxiv.org/pdf/2210.03057.pdf) | [Link](./material/20231206-Symbolic%20Reasoning/VIDY%20presentation%20Dec%207th.pdf) | [Link](./material/20231206-Symbolic%20Reasoning/)            |
| 2023/12/27   | Algorithm    | Uncertainty quantification                       | [Dropout as Bayesian approximation: Representing Model Uncertainty in Deep Learning](https://arxiv.org/pdf/1506.02142.pdf), [Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles](https://arxiv.org/pdf/1612.01474.pdf) |                                                              | [Link](./material/20231227-Uncertainty%20quantification/)    |
| 2024/01/10   | Applications | Drug discovery                                   | [Geometric Latent Diffusion Models for 3D Molecule Generation](https://arxiv.org/abs/2305.01140) | [Link](./material/20240110-Geometric%20Diffusion%20Model/Geometric%20Latent%20Diffusion%20Model.pdf) | [Link](./material/20240110-Geometric%20Diffusion%20Model/)   |
| 2024/01/24   | Applications | Program synthesis                                | [Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=oavgGaMAAAAJ&sortby=pubdate&citation_for_view=oavgGaMAAAAJ:nRpfm8aw39MC) | [Link](./material/20240124-Learning%20to%20Execute/VIDY%20Presentation%20Xingjian.pdf) | [Link](./material/20240124-Learning%20to%20Execute/)         |
| 2024/02/14   | Applications | 3-D molecule                                     | [SchNet: A continuous-filter convolutional neural network for modeling quantum interactions](https://arxiv.org/pdf/1706.08566.pdf), [SPHERICAL MESSAGE PASSING FOR 3D MOLECULAR GRAPHS](https://arxiv.org/pdf/2102.05013.pdf) |                                                              | [Link](./material/20240214-3D%20Molecule/VIDY%20presentation_Jianpeng%20Chen_3D%20molecule.pdf) |
| 2024/02/28   | Algorithm    | Transfer learning, domain adaptation             | [Graph Optimal Transport for Cross-Domain Alignment](http://proceedings.mlr.press/v119/chen20e/chen20e.pdf) | [Link](./material/20240228-Transfer%20Learning/)             | [Link](./material/20240228-Transfer%20Learning/VIDY_presentation_ngoc.pdf) |
| 2024/03/06   | Applications | Transportation network/spatial-temporal learning | [ROLAND: Graph Learning Framework for Dynamic Graphs](https://arxiv.org/abs/2208.07239) |                                                              |                                                              |
| 2024/03/27   | Theory       | Generalization                                   | [UML](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf) Chapter 2 & 3 (ERM & PAC Learning) |                                                              | [Link](./material/20240327-UML%20ch2&3/VIDY_presentation_UMLCh23.pdf) |
| 2024/04/17   | Theory       | Generalization                                   | [UML](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf) Chapter 4 (Uniform Convergence) | [Link](./material/20240417-UML%20ch4/VIDY_UMLCh4.pdf)        | [Link](./material/20240417-UML%20ch4)                        |
| 2024/05/29   | Theory       | Generalization                                   | [UML](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf) Chapter 6 (VC Dimension) | [Link](./material/20240529-UML%20ch6/VC_Dimension.pdf)       | [Link](material/20240529-UML%20ch6/)                         |
| 2024/06/12   | Theory       | Generalization                                   | [UML](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf)[Chapter 30 & Stronger generalization bounds for deep nets via a compression approach](https://arxiv.org/pdf/1802.05296.pdf) |                                                              |                                                              |
| 2024/06/26   | Theory       | Generalization                                   | [UML](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf)[Chapter 31 & A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks](https://arxiv.org/abs/1707.09564) |                                                              |                                                              |
| 2024/07/10   | Algorithm    | Distributed algorithms                           | [MocoSFL: enabling cross-client collaborative self-supervised learning](https://openreview.net/forum?id=2QGJXyMNoPz) | [Link](./material/20240710-Distributed%20algorithms%VIDY_0710.pdf) | [Link](./material/20240710-Distributed%20algorithms)         |
| 2024/07/24   | Theory       | Generalization                                   | [UML](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf) Chapter 26 (Rademacher Complexity) |                                                              |                                                              |

## Group Reading Roles

The group reading is organized around the different “roles” students play each week: Presenter, Challenger, Discussant, Summarizer, and Hacker. Roles define the lens through which students read the paper.

Note: Each student should sign up for at least **1 presenter, 2 challengers, and 1 summarizer** role. You can sign up as a 3rd challenger only if all papers have at least 2 challengers.

* **Presenter:** Create the main presentation, describing the motivation, problem definition, method, and experimental findings of this paper.

* **Challenger:** Prepare 2-3 critical but not necessarily negative questions.  Please note that this role does not require going over related work, and is not an exhaustive list of all arguments you can think of. What are your reactions to the Papers and why? What can the authors do to improve the Paper? Learning how to read other Papers with the **critical eye of a good**.

* **Discussant:** Propose an imaginary follow-up project (e.g., novel theorem, methodology, and applications) that has now become possible due to the existence and success of the current paper. Think about **what is beyond** this paper and the what are the connections with other papers? Come up with several discussion points (e.g., constructive ways to improve the paper, the relevance of the paper to academia and society).

* **Summarizer:** Your responsibility is to help the audience **understand the papers better**. You should have read the papers carefully, taken time to understand their contributions, strengths and weaknesses, and what the audience needs to know about them. After the meeting, you should summarize the meeting discussion in 2-3 slides by including the paper's main message, contributions, limitations, and future directions. Collect all the reading materials (papers, presentation slides, summary slides) and upload them to our shared repository.

* **Hacker (optional):** Implement a small part of the paper on a small dataset or toy problem, or any other simplified version of the paper. Share a Jupyter Notebook with the code of the algorithm with the reading group. This role is optional, i.e., you can declare if you would like to be a Hacker as an alternative for a specific week, for example, when you like the topic and would like to get hands-on experience, but no student will necessarily have to complete this role. Please do not simply download and run an existing implementation, make an effort to at least implement a core method from the paper. After all, your code does not have to be bug-free or run perfectly in all scenarios. Also, you are welcome to use (and give credit to) an existing implementation for “backbone” code (e.g. model building, data loading, training loop, etc.).
